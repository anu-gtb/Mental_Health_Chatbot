{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer,TrainingArguments,Trainer,TextDataset,DataCollatorForLanguageModeling\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='gpt2'\n",
    "tokenizer=GPT2Tokenizer.from_pretrained(model_name)\n",
    "model=GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20855, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hi</td>\n",
       "      <td>Hello there. Tell me how are you feeling today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey</td>\n",
       "      <td>Hi there. What brings you here today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>Hi there. How are you feeling today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hi there</td>\n",
       "      <td>Great to see you. How do you feel currently?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hello</td>\n",
       "      <td>Hello there. Glad to see you. What is going on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         Questions  \\\n",
       "0           0                Hi   \n",
       "1           1               Hey   \n",
       "2           2  Is anyone there?   \n",
       "3           3          Hi there   \n",
       "4           4             Hello   \n",
       "\n",
       "                                             Answers  \n",
       "0    Hello there. Tell me how are you feeling today?  \n",
       "1              Hi there. What brings you here today?  \n",
       "2               Hi there. How are you feeling today?  \n",
       "3       Great to see you. How do you feel currently?  \n",
       "4  Hello there. Glad to see you. What is going on...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20850</th>\n",
       "      <td>20850</td>\n",
       "      <td>Okay,Thanks a lot</td>\n",
       "      <td>Thank you for visiting. Have a nice day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20851</th>\n",
       "      <td>20851</td>\n",
       "      <td>Got it,Thank you</td>\n",
       "      <td>Thank you for visiting. Have a nice day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20852</th>\n",
       "      <td>20852</td>\n",
       "      <td>Got it,Thank you so much</td>\n",
       "      <td>Thank you for visiting. Have a nice day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20853</th>\n",
       "      <td>20853</td>\n",
       "      <td>Got it,Thanks</td>\n",
       "      <td>Thank you for visiting. Have a nice day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20854</th>\n",
       "      <td>20854</td>\n",
       "      <td>Got it,Thanks a lot</td>\n",
       "      <td>Thank you for visiting. Have a nice day!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                 Questions  \\\n",
       "20850       20850         Okay,Thanks a lot   \n",
       "20851       20851          Got it,Thank you   \n",
       "20852       20852  Got it,Thank you so much   \n",
       "20853       20853             Got it,Thanks   \n",
       "20854       20854       Got it,Thanks a lot   \n",
       "\n",
       "                                        Answers  \n",
       "20850  Thank you for visiting. Have a nice day!  \n",
       "20851  Thank you for visiting. Have a nice day!  \n",
       "20852  Thank you for visiting. Have a nice day!  \n",
       "20853  Thank you for visiting. Have a nice day!  \n",
       "20854  Thank you for visiting. Have a nice day!  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20855 entries, 0 to 20854\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  20855 non-null  int64 \n",
      " 1   Questions   20855 non-null  object\n",
      " 2   Answers     20852 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 488.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path,tokenizer,block_size=1024):\n",
    "    train_data=TextDataset(tokenizer=tokenizer,file_path=file_path,block_size=block_size)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer,mlm=False):\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=mlm)\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_file_path,model_name,output_dir,overwrite_output_dir,train_batch_size,epochs):\n",
    "    tokenizer=GPT2Tokenizer.from_pretrained(model_name)\n",
    "    train_data=load_dataset(train_file_path,tokenizer)\n",
    "    \n",
    "    data_collator=load_data_collator(tokenizer)\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    model=GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.save_pretrained(output_dir)\n",
    "    \n",
    "    training_arguments=TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        logging_dir='./log',\n",
    "        logging_steps=50,\n",
    "        save_steps=150,\n",
    "        logging_first_step=True,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    trainer=Trainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data\n",
    "    )\n",
    "    hist=trainer.train()\n",
    "    trainer.save_model()\n",
    "    return trainer,hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path='C:\\\\Users\\\\DELL\\\\Documents\\\\CWB_HACK\\\\500_.csv'\n",
    "model_name=model_name\n",
    "output_dir='./model'\n",
    "overwrite_output_dir=True\n",
    "train_batch_size=3\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37f389d9373466a8bb045e200a3bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4527, 'grad_norm': 5.69218111038208, 'learning_rate': 0.0009979591836734693, 'epoch': 0.02}\n",
      "{'loss': 3.5727, 'grad_norm': 1.2075151205062866, 'learning_rate': 0.0008979591836734694, 'epoch': 1.02}\n",
      "{'loss': 2.6217, 'grad_norm': 1.04498291015625, 'learning_rate': 0.0007959183673469387, 'epoch': 2.04}\n",
      "{'loss': 1.9433, 'grad_norm': 1.0610477924346924, 'learning_rate': 0.0006938775510204082, 'epoch': 3.06}\n",
      "{'loss': 1.3459, 'grad_norm': 1.0818220376968384, 'learning_rate': 0.0005918367346938776, 'epoch': 4.08}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_file_path, model_name, output_dir, overwrite_output_dir, train_batch_size, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m training_arguments\u001b[38;5;241m=\u001b[39mTrainingArguments(\n\u001b[0;32m     13\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m     14\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39moverwrite_output_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m trainer\u001b[38;5;241m=\u001b[39mTrainer(\n\u001b[0;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     27\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_arguments,\n\u001b[0;32m     28\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     29\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 31\u001b[0m hist\u001b[38;5;241m=\u001b[39m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer,hist\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist=train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    train_batch_size=train_batch_size,\n",
    "    epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run-time:  255.8535\n",
      "Loss:  0.6654784452915191\n"
     ]
    }
   ],
   "source": [
    "print('Run-time: ',hist[1].metrics['train_runtime'])\n",
    "print('Loss: ',hist[1].metrics['train_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
